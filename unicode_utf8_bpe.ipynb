{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Unicode, UTF-8, and Byte-Pair Encoding (BPE)\n",
    "\n",
    "This notebook illustrates:\n",
    "1. How Unicode strings are represented\n",
    "2. Converting Unicode to UTF-8 byte sequences\n",
    "3. How byte-level BPE tokenization works\n",
    "4. Encoding and decoding text using BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unicode-intro",
   "metadata": {},
   "source": [
    "## 1. Unicode Basics\n",
    "\n",
    "**Unicode** is a standard that assigns a unique number (called a **code point**) to every character across all languages and symbols.\n",
    "\n",
    "- Code points are written as `U+XXXX` (hexadecimal)\n",
    "- Python strings are Unicode by default (Python 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unicode-examples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unicode characters and their code points\n",
    "examples = ['A', 'z', '5', ' ', '√©', '‰∏≠', 'üöÄ', '‚Üí']\n",
    "\n",
    "print(\"Char | Code Point | Decimal\")\n",
    "print(\"-\" * 30)\n",
    "for char in examples:\n",
    "    code_point = ord(char)  # Get Unicode code point\n",
    "    print(f\"  {char}  |   U+{code_point:04X}   |  {code_point}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utf8-intro",
   "metadata": {},
   "source": [
    "## 2. UTF-8 Encoding\n",
    "\n",
    "**UTF-8** is a variable-length encoding that converts Unicode code points to bytes:\n",
    "\n",
    "| Code Point Range | Bytes | Byte Pattern |\n",
    "|-----------------|-------|---------------|\n",
    "| U+0000 - U+007F | 1 | `0xxxxxxx` |\n",
    "| U+0080 - U+07FF | 2 | `110xxxxx 10xxxxxx` |\n",
    "| U+0800 - U+FFFF | 3 | `1110xxxx 10xxxxxx 10xxxxxx` |\n",
    "| U+10000 - U+10FFFF | 4 | `11110xxx 10xxxxxx 10xxxxxx 10xxxxxx` |\n",
    "\n",
    "ASCII characters (0-127) use just 1 byte, making UTF-8 backward compatible with ASCII."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utf8-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Unicode strings to UTF-8 bytes\n",
    "text = \"Hello, ‰∏ñÁïå! üåç\"\n",
    "\n",
    "# Encode to UTF-8 bytes\n",
    "utf8_bytes = text.encode('utf-8')\n",
    "\n",
    "print(f\"Original string: {text}\")\n",
    "print(f\"String length (characters): {len(text)}\")\n",
    "print(f\"UTF-8 bytes: {utf8_bytes}\")\n",
    "print(f\"Byte length: {len(utf8_bytes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utf8-breakdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed breakdown of UTF-8 encoding per character\n",
    "print(\"Character-by-character UTF-8 breakdown:\\n\")\n",
    "print(\"Char | UTF-8 Bytes (hex) | UTF-8 Bytes (decimal)\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for char in text:\n",
    "    char_bytes = char.encode('utf-8')\n",
    "    hex_repr = ' '.join(f'{b:02X}' for b in char_bytes)\n",
    "    dec_repr = ' '.join(f'{b:3d}' for b in char_bytes)\n",
    "    display_char = char if char != ' ' else '(space)'\n",
    "    print(f\"  {display_char:4} |  {hex_repr:17} | {dec_repr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utf8-decode",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding UTF-8 bytes back to a string\n",
    "decoded_text = utf8_bytes.decode('utf-8')\n",
    "print(f\"Decoded string: {decoded_text}\")\n",
    "print(f\"Roundtrip successful: {text == decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gpt2-byte-encoding",
   "metadata": {},
   "source": [
    "## 3. GPT-2 Style Byte Encoding\n",
    "\n",
    "GPT-2 (and many modern tokenizers) work at the **byte level**, but they map each byte (0-255) to a printable Unicode character. This avoids issues with non-printable bytes.\n",
    "\n",
    "The mapping:\n",
    "- Printable ASCII characters (`!` to `~`, plus space) map to themselves\n",
    "- Other bytes map to characters starting at `U+0100` (ƒÄ, ƒÅ, ƒÇ, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gpt2-mapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns a mapping from bytes (0-255) to unicode characters.\n",
    "    This is the GPT-2 byte encoding scheme.\n",
    "    \"\"\"\n",
    "    # Printable ASCII ranges that map to themselves\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\") + 1))  # 33-126\n",
    "    bs += list(range(ord(\"¬°\"), ord(\"¬¨\") + 1))  # 161-172\n",
    "    bs += list(range(ord(\"¬Æ\"), ord(\"√ø\") + 1))  # 174-255\n",
    "    \n",
    "    cs = bs[:]\n",
    "    \n",
    "    # Map remaining bytes (0-32, 127-160, 173) to characters starting at 256\n",
    "    n = 0\n",
    "    for b in range(256):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(256 + n)\n",
    "            n += 1\n",
    "    \n",
    "    cs = [chr(c) for c in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "byte_encoder = bytes_to_unicode()\n",
    "byte_decoder = {v: k for k, v in byte_encoder.items()}\n",
    "\n",
    "print(\"Sample byte-to-character mappings:\")\n",
    "print(\"\\nByte | Char | Notes\")\n",
    "print(\"-\" * 35)\n",
    "sample_bytes = [0, 10, 32, 65, 97, 127, 128, 200, 255]\n",
    "for b in sample_bytes:\n",
    "    char = byte_encoder[b]\n",
    "    note = \"\"\n",
    "    if b == 0: note = \"(null byte)\"\n",
    "    elif b == 10: note = \"(newline)\"\n",
    "    elif b == 32: note = \"(space)\"\n",
    "    elif b == 65: note = \"(ASCII 'A')\"\n",
    "    elif b == 97: note = \"(ASCII 'a')\"\n",
    "    elif b == 127: note = \"(DEL)\"\n",
    "    print(f\" {b:3d} |  {char}   | {note}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gpt2-encode-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode text to GPT-2 style byte representation\n",
    "def text_to_gpt2_bytes(text):\n",
    "    \"\"\"Convert text to GPT-2's byte representation.\"\"\"\n",
    "    utf8_bytes = text.encode('utf-8')\n",
    "    return ''.join(byte_encoder[b] for b in utf8_bytes)\n",
    "\n",
    "def gpt2_bytes_to_text(gpt2_str):\n",
    "    \"\"\"Convert GPT-2 byte representation back to text.\"\"\"\n",
    "    byte_values = bytes([byte_decoder[c] for c in gpt2_str])\n",
    "    return byte_values.decode('utf-8')\n",
    "\n",
    "# Example\n",
    "original = \"Hello, ‰∏ñÁïå! üåç\"\n",
    "gpt2_repr = text_to_gpt2_bytes(original)\n",
    "recovered = gpt2_bytes_to_text(gpt2_repr)\n",
    "\n",
    "print(f\"Original:         {original}\")\n",
    "print(f\"GPT-2 byte repr:  {gpt2_repr}\")\n",
    "print(f\"Recovered:        {recovered}\")\n",
    "print(f\"\\nNotice how multi-byte UTF-8 characters become multiple GPT-2 tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bpe-intro",
   "metadata": {},
   "source": [
    "## 4. Byte-Pair Encoding (BPE)\n",
    "\n",
    "**BPE** is a compression algorithm adapted for tokenization:\n",
    "\n",
    "1. Start with a vocabulary of individual bytes (or characters)\n",
    "2. Find the most frequent adjacent pair of tokens\n",
    "3. Merge that pair into a new token\n",
    "4. Repeat until vocabulary reaches desired size\n",
    "\n",
    "This creates a vocabulary of subword units that balances:\n",
    "- Common words as single tokens\n",
    "- Rare words split into meaningful pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bpe-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_pair_counts(token_sequences):\n",
    "    \"\"\"Count frequency of adjacent token pairs.\"\"\"\n",
    "    pairs = Counter()\n",
    "    for seq in token_sequences:\n",
    "        for i in range(len(seq) - 1):\n",
    "            pairs[(seq[i], seq[i + 1])] += 1\n",
    "    return pairs\n",
    "\n",
    "def merge_pair(token_sequences, pair, new_token):\n",
    "    \"\"\"Merge all occurrences of a pair into a new token.\"\"\"\n",
    "    new_sequences = []\n",
    "    for seq in token_sequences:\n",
    "        new_seq = []\n",
    "        i = 0\n",
    "        while i < len(seq):\n",
    "            if i < len(seq) - 1 and (seq[i], seq[i + 1]) == pair:\n",
    "                new_seq.append(new_token)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_seq.append(seq[i])\n",
    "                i += 1\n",
    "        new_sequences.append(new_seq)\n",
    "    return new_sequences\n",
    "\n",
    "def train_bpe(text, num_merges):\n",
    "    \"\"\"Train BPE on text and return vocabulary and merge rules.\"\"\"\n",
    "    # Start with individual characters as tokens\n",
    "    tokens = [list(word) for word in text.split()]\n",
    "    \n",
    "    # Initial vocabulary is all unique characters\n",
    "    vocab = set(char for word in tokens for char in word)\n",
    "    merges = []\n",
    "    \n",
    "    print(f\"Initial vocabulary size: {len(vocab)}\")\n",
    "    print(f\"Initial tokens: {tokens[:3]}...\\n\")\n",
    "    \n",
    "    for i in range(num_merges):\n",
    "        pairs = get_pair_counts(tokens)\n",
    "        if not pairs:\n",
    "            break\n",
    "        \n",
    "        # Find most frequent pair\n",
    "        best_pair = max(pairs, key=pairs.get)\n",
    "        new_token = best_pair[0] + best_pair[1]\n",
    "        \n",
    "        print(f\"Merge {i+1}: '{best_pair[0]}' + '{best_pair[1]}' -> '{new_token}' (count: {pairs[best_pair]})\")\n",
    "        \n",
    "        # Merge the pair\n",
    "        tokens = merge_pair(tokens, best_pair, new_token)\n",
    "        vocab.add(new_token)\n",
    "        merges.append(best_pair)\n",
    "    \n",
    "    print(f\"\\nFinal vocabulary size: {len(vocab)}\")\n",
    "    return vocab, merges, tokens\n",
    "\n",
    "# Train BPE on sample text\n",
    "sample_text = \"low lower lowest low lower lowest newer newest wide wider widest\"\n",
    "vocab, merges, final_tokens = train_bpe(sample_text, num_merges=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bpe-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show final tokenization\n",
    "print(\"Final tokenization:\")\n",
    "print(final_tokens)\n",
    "\n",
    "print(\"\\nMerge rules (in order):\")\n",
    "for i, (a, b) in enumerate(merges):\n",
    "    print(f\"  {i+1}. '{a}' + '{b}' -> '{a}{b}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bpe-encoding",
   "metadata": {},
   "source": [
    "## 5. BPE Encoding (Tokenization)\n",
    "\n",
    "To encode new text with a trained BPE model:\n",
    "1. Split text into characters (or bytes)\n",
    "2. Apply merge rules in the order they were learned\n",
    "3. Map resulting tokens to integer IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bpe-encode-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe_encode(text, merges):\n",
    "    \"\"\"Encode text using learned BPE merges.\"\"\"\n",
    "    words = text.split()\n",
    "    encoded_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        tokens = list(word)  # Start with characters\n",
    "        \n",
    "        # Apply merges in order\n",
    "        for pair in merges:\n",
    "            i = 0\n",
    "            new_tokens = []\n",
    "            while i < len(tokens):\n",
    "                if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == pair:\n",
    "                    new_tokens.append(tokens[i] + tokens[i + 1])\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "            tokens = new_tokens\n",
    "        \n",
    "        encoded_words.append(tokens)\n",
    "    \n",
    "    return encoded_words\n",
    "\n",
    "# Test encoding\n",
    "test_words = [\"low\", \"lower\", \"lowest\", \"new\", \"newer\", \"unknown\"]\n",
    "print(\"BPE tokenization results:\\n\")\n",
    "for word in test_words:\n",
    "    tokens = bpe_encode(word, merges)[0]\n",
    "    print(f\"  '{word}' -> {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bpe-decoding",
   "metadata": {},
   "source": [
    "## 6. BPE Decoding\n",
    "\n",
    "Decoding is simple: concatenate tokens and convert bytes back to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bpe-decode-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe_decode(token_sequences):\n",
    "    \"\"\"Decode BPE tokens back to text.\"\"\"\n",
    "    words = [''.join(tokens) for tokens in token_sequences]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Round-trip test\n",
    "original_text = \"lower newest wider\"\n",
    "encoded = bpe_encode(original_text, merges)\n",
    "decoded = bpe_decode(encoded)\n",
    "\n",
    "print(f\"Original: '{original_text}'\")\n",
    "print(f\"Encoded:  {encoded}\")\n",
    "print(f\"Decoded:  '{decoded}'\")\n",
    "print(f\"Roundtrip OK: {original_text == decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "byte-level-bpe",
   "metadata": {},
   "source": [
    "## 7. Byte-Level BPE (Putting It All Together)\n",
    "\n",
    "Modern tokenizers like GPT-2 combine:\n",
    "1. **UTF-8 encoding** - Convert Unicode text to bytes\n",
    "2. **Byte-to-character mapping** - Make all bytes printable\n",
    "3. **BPE** - Learn subword merges on the byte representation\n",
    "\n",
    "This allows handling ANY text (any language, emoji, binary data) with a fixed vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "byte-level-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete byte-level BPE pipeline demonstration\n",
    "def demonstrate_byte_level_bpe(text):\n",
    "    print(f\"Input text: {text}\")\n",
    "    print(f\"Character count: {len(text)}\\n\")\n",
    "    \n",
    "    # Step 1: UTF-8 encode\n",
    "    utf8_bytes = text.encode('utf-8')\n",
    "    print(f\"Step 1 - UTF-8 bytes: {list(utf8_bytes)}\")\n",
    "    print(f\"Byte count: {len(utf8_bytes)}\\n\")\n",
    "    \n",
    "    # Step 2: Map to printable characters (GPT-2 style)\n",
    "    gpt2_chars = ''.join(byte_encoder[b] for b in utf8_bytes)\n",
    "    print(f\"Step 2 - GPT-2 byte chars: {gpt2_chars}\")\n",
    "    print(f\"Char count: {len(gpt2_chars)}\\n\")\n",
    "    \n",
    "    # Step 3: This is where BPE would tokenize\n",
    "    print(\"Step 3 - BPE would merge frequent pairs into tokens\")\n",
    "    print(\"(Using pre-trained merges from vocabulary)\\n\")\n",
    "    \n",
    "    # Decoding reverses the process\n",
    "    print(\"Decoding:\")\n",
    "    recovered_bytes = bytes([byte_decoder[c] for c in gpt2_chars])\n",
    "    recovered_text = recovered_bytes.decode('utf-8')\n",
    "    print(f\"  GPT-2 chars -> bytes -> UTF-8 decode -> '{recovered_text}'\")\n",
    "\n",
    "# Try with various Unicode text\n",
    "demonstrate_byte_level_bpe(\"Hello ‰∏ñÁïå üéâ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Step | Process | Purpose |\n",
    "|------|---------|--------|\n",
    "| 1 | Unicode string | Human-readable text |\n",
    "| 2 | UTF-8 encoding | Convert to bytes (1-4 bytes per char) |\n",
    "| 3 | Byte-to-char mapping | Make all 256 byte values printable |\n",
    "| 4 | BPE tokenization | Compress into subword tokens |\n",
    "| 5 | Token IDs | Integer indices for the model |\n",
    "\n",
    "**Key benefits of byte-level BPE:**\n",
    "- Handles ANY Unicode text without \"unknown\" tokens\n",
    "- Efficient vocabulary (common words = single tokens)\n",
    "- Graceful degradation (rare text = more tokens, but still works)\n",
    "- Language-agnostic (no language-specific preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final interactive example - try your own text!\n",
    "your_text = \"Try any text here: caf√©, na√Øve, Êó•Êú¨Ë™û, emoji üé≠\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BYTE-LEVEL ENCODING BREAKDOWN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for char in your_text[:20]:  # First 20 chars\n",
    "    utf8 = char.encode('utf-8')\n",
    "    gpt2 = ''.join(byte_encoder[b] for b in utf8)\n",
    "    display = char if char != ' ' else '‚ê£'\n",
    "    print(f\"{display:2} -> UTF-8: {list(utf8):20} -> GPT-2: {gpt2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
